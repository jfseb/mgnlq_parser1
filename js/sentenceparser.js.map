{"version":3,"sources":["/projects/nodejs/botbuilder/abot_stringdist/src/../src/sentenceparser.ts"],"names":[],"mappings":"AAAA,YAAY,CAAA;;AAEZ,+FAA+F;AAC/F,0EAA0E;AAE1E,uCAAwF;AAExF,gCAAgC;AAEhC,yCAAyC;AAGzC,MAAM,QAAQ,GAAG,KAAK,CAAC,gBAAgB,CAAC,CAAC;AAEzC,yCAAyC;AACzC,6BAA6B;AAI3B,IAAI,WAAW,GAAG,UAAU,CAAC,WAAW,CAAC;AACzC,IAAI,KAAK,GAAG,UAAU,CAAC,KAAK,CAAC;AAC7B,IAAI,MAAM,GAAG,UAAU,CAAC,MAAM,CAAC;AAIjC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EA2DE;AAGF,6CAAgD;AAE9C,IAAI,UAAU,GAAG,WAAW,CAAC,EAAC,IAAI,EAAE,YAAY,EAAE,OAAO,EAAE,KAAK,EAAC,CAAC,CAAC;AAEnE,UAAU,CAAC,KAAK,GAAG,KAAK,CAAC,OAAO,CAAC;AAEjC,qCAAwC;AACxC,iFAAiF;AACjF,IAAI,SAAS,GAAG,MAAM,CAAC,IAAI,CAAC,eAAC,CAAC,CAAC,GAAG,CAAC,GAAG,IAAI,eAAC,CAAC,GAAG,CAAC,CAAC,CAAC;AAGpD,mBAA0B,CAAkB,EAAE,KAAc,EAAE,CAAO;IACnE,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC;QACZ,MAAM,IAAI,KAAK,CAAC,qBAAqB,GAAG,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;IAC7D,CAAC;IACD,yCAAyC;IACzC,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,qBAAO,CAAC,QAAQ,CAAC,QAAQ,CAAC,CAAC,CAAC;QAClD,MAAM,CAAC,EAAE,KAAK,EAAG,KAAK,EAAG,WAAW,EAAG,KAAK,EAAE,MAAM,EAAG,CAAC,EAAE,SAAS,EAAG,CAAC,CAAC,WAAW,CAAC,CAAC,SAAS,EAAE,CAAC;IACnG,CAAC;IAAA,CAAC;IACF,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,GAAG,CAAC,CAAC,CAAC;QAC5B,iCAAiC;QACjC,MAAM,CAAC,EAAE,KAAK,EAAG,MAAM,EAAG,WAAW,EAAG,KAAK,EAAE,MAAM,EAAG,CAAC,EAAE,SAAS,EAAG,CAAC,CAAC,OAAO,CAAC,CAAC,SAAS,EAAE,CAAC;IAChG,CAAC;IACD,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,GAAG,CAAC,CAAC,CAAC;QAC5B,iCAAiC;QACjC,MAAM,CAAC,EAAE,KAAK,EAAG,KAAK,EAAG,WAAW,EAAG,KAAK,EAAE,MAAM,EAAG,CAAC,EAAE,SAAS,EAAG,CAAC,CAAC,SAAS,CAAC,CAAC,SAAS,EAAE,CAAC;IACjG,CAAC;IACD,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,GAAG,CAAC,CAAC,CAAC;QAC5B,iCAAiC;QACjC,MAAM,CAAC,EAAE,KAAK,EAAG,KAAK,EAAG,WAAW,EAAG,KAAK,EAAE,MAAM,EAAG,CAAC,EAAE,SAAS,EAAG,CAAC,CAAC,OAAO,CAAC,CAAC,SAAS,EAAE,CAAC;IAC/F,CAAC;IACD,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,GAAG,CAAC,CAAC,CAAC;QAC5B,iCAAiC;QACjC,IAAI,GAAG,GAAG,CAAC,CAAC,aAAa,CAAC,WAAW,EAAE,CAAC;QACxC,IAAI,QAAQ,GAAG,GAAG,CAAC,OAAO,CAAC,IAAI,EAAC,GAAG,CAAC,CAAC;QACrC,oCAAoC;QACpC,gDAAgD;QAChD,mDAAmD;QACnD,0DAA0D;QAC1D,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC;YACjB,4CAA4C;YAC5C,MAAM,IAAI,KAAK,CAAC,+BAA+B,GAAG,CAAC,CAAC,aAAa,CAAC,CAAC;QACrE,CAAC;QACD,kDAAkD;QAClD,MAAM,CAAC,EAAE,KAAK,EAAG,CAAC,CAAC,aAAa,EAAE,MAAM,EAAG,CAAC,EAAE,WAAW,EAAG,KAAK,EAAE,SAAS,EAAG,CAAC,CAAC,QAAQ,CAAC,CAAC,SAAS,EAAE,CAAC;IACzG,CAAC;IACD,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,GAAG,CAAC,CAAC,CAAC;QAC5B,IAAI,GAAG,GAAG,CAAC,CAAC,aAAa,CAAC,WAAW,EAAE,CAAC;QACxC,IAAI,QAAQ,GAAG,GAAG,CAAC,OAAO,CAAC,IAAI,EAAC,GAAG,CAAC,CAAC;QACrC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC;YACjB,OAAO,CAAC,GAAG,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC;YAC1C,MAAM,IAAI,KAAK,CAAC,+BAA+B,GAAG,CAAC,CAAC,aAAa,CAAC,CAAC;YACnE,mBAAmB;QACrB,CAAC;QACD,MAAM,CAAC,EAAE,KAAK,EAAG,CAAC,CAAC,aAAa,EAAE,MAAM,EAAG,CAAC,EAAE,WAAW,EAAG,KAAK,EAAE,SAAS,EAAG,CAAC,CAAC,QAAQ,CAAC,CAAC,SAAS,EAAE,CAAC;IACzG,CAAC;IACD,EAAE,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,QAAQ,KAAK,GAAG,CAAC,CAAC,CAAC;QAC5B,IAAI,GAAG,GAAG,CAAC,CAAC,aAAa,CAAC,WAAW,EAAE,CAAC;QACxC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;YACZ,OAAO,CAAC,GAAG,CAAC,+BAA+B,GAAG,CAAC,CAAC,aAAa,CAAC,CAAC;YAC/D,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;QACnB,CAAC;QACD,MAAM,CAAC,EAAE,KAAK,EAAG,CAAC,CAAC,aAAa,EAAE,MAAM,EAAG,CAAC,EAAE,WAAW,EAAG,KAAK,EAAE,SAAS,EAAG,CAAC,CAAC,GAAG,CAAC,CAAC,SAAS,EAAE,CAAC;IACpG,CAAC;IACD,MAAM,IAAI,KAAK,CAAC,gBAAgB,GAAG,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;AACxD,CAAC;AAtDD,8BAsDC;AAED;IAAA;QACG,aAAQ,GAAG,UAAS,QAA6B;YAChD,yCAAyC;YACzC,MAAM,CAAC,QAAQ,CAAC,GAAG,CAAE,CAAC,CAAC,EAAC,KAAK;gBACxB,IAAI,CAAC,GAAI,SAAS,CAAC,CAAC,EAAE,KAAK,EAAE,eAAC,CAAC,CAAC;gBACjC,QAAQ,CAAC,gBAAgB,GAAG,KAAK,GAAG,KAAK,GAAG,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC/D,MAAM,CAAC,CAAC,CAAC;YACb,CAAC,CAAC,CAAC;QACL,CAAC,CAAA;IACH,CAAC;CAAA;AAAA,CAAC;AAEF;IACE,MAAM,CAAC,IAAI,MAAM,EAAE,CAAC;AACtB,CAAC;AAFD,4BAEC;AAEC;;;;;;;;;;;EAWA;AACE,IAAI,WAAW,GAAG,IAAI,KAAK,CAAC,SAAS,CAAC,CAAC;AAoDxC,kCAAW;AAjDd,eAAe,MAAc,EAAE,SAAkB;IAC/C,MAAM,MAAM,GAAG,IAAI,YAAY,CAAC,YAAY,CAAC,MAAM,CAAC,CAAC;IACrD,IAAI,GAAG,GAAG,MAAM,CAAC,SAAS,CAAC,EAAE,CAAC;IAC7B,EAAE,CAAC,CAAC,MAAM,CAAC,MAAM,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;QAC9B,MAAM,IAAI,KAAK,CAAC,yBAAyB,GAAG,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,CAAC;IAC7E,CAAC;IACD,MAAM,CAAC,GAAG,CAAC;AACb,CAAC;AA2CE,sBAAK;AAtCP,CAAC;AAIF,6BAAoC,CAAU,EAAE,KAAuB,EAAE,KAAW;IAClF,IAAI,GAAG,GAAG,iBAAM,CAAC,aAAa,CAAC,CAAC,EAAE,KAAK,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;IACtD,QAAQ,CAAC,cAAa,QAAQ,GAAG,IAAI,CAAC,SAAS,CAAC,GAAG,EAAE,SAAS,EAAE,CAAC,CAAC,CAAA,CAAA,CAAC,CAAC,CAAC;IACrE,IAAI,IAAI,GAAG,MAAM,CAAC,MAAM,CAAC,EAAE,EAAE,GAAG,CAAqB,CAAC;IACtD,IAAI,CAAC,IAAI,GAAG,GAAG,CAAC,SAAS,CAAC,GAAG,CAAC,CAAC,QAAQ,EAAE,KAAK;QAC5C,IAAI,YAAY,GAAG,QAAQ,EAAE,CAAC,QAAQ,CAAC,QAAQ,CAAC,CAAC;QACjD,QAAQ,CAAE;YACR,IAAI,QAAQ,GAAG,YAAY,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,MAAM,KAAM,IAAI,MAAM,KAAK,CAAC,CAAC,KAAK,KAAK,CAAC,CAAC,MAAM,IAAI,CAAC,CAAC,MAAM,CAAC,aAAa,IAAI,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,UAAU,CAAC,CAAC,GAAG,CAAE,CAAC;YACpK,MAAM,CAAC,WAAW,GAAG,KAAK,GAAG,OAAO,GAAG,QAAQ,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;QAAA,CAAC,CAC7D,CAAC;QACF,wGAAwG;QACxG,IAAI,CAAC;YACH,IAAI,GAAG,GAAG,KAAK,CAAC,YAAY,EAAE,eAAe,CAAC,CAAC;YAC/C,QAAQ,CAAC;gBACP,MAAM,CAAC,QAAQ,GAAG,KAAK,GAAG,OAAO,GAAG,GAAG,CAAC,SAAS,CAAC,GAAG,CAAC,CAAC;YACzD,CAAC,CAAC,CAAC;YACH,MAAM,CAAC,GAAG,CAAC;QACb,CAAC;QAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YACX,QAAQ,CAAC,MAAI,kBAAkB,GAAG,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;YAClD,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,MAAM,IAAI,EAAE,CAAC;YAChC,QAAQ,CAAC,cAAc,GAAG,CAAC,CAAC,QAAQ,EAAE,CAAC,CAAC;YAExC,IAAI,CAAC,MAAM,CAAC,KAAK,CAAC,GAAG;gBACnB,QAAQ,EAAG,uBAAe;gBAC1B,IAAI,EAAG,CAAC,CAAC,QAAQ,EAAE,CAAC,KAAK,CAAC,aAAa,CAAC,CAAC,CAAC,CAAC;aACtB,CAAC;QAC1B,CAAC;QACD,MAAM,CAAC,SAAS,CAAC;IACnB,CAAC,CAAC,CAAC;IACH,MAAM,CAAC,IAAI,CAAC;AACd,CAAC;AA9BD,kDA8BC","file":"sentenceparser.js","sourcesContent":["'use strict'\r\n\r\n// based on: http://en.wikibooks.org/wiki/Algorithm_implementation/Strings/Levenshtein_distance\r\n// and:  http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\r\n\r\nimport { ErBase as ErBase, Sentence as Sentence, IFErBase as IFErBase } from 'mgnlq_er';\r\n\r\nimport * as debug from 'debugf';\r\n\r\nimport * as SelectParser from './parser';\r\n\r\n\r\nconst debuglog = debug('sentenceparser');\r\n\r\nimport * as chevrotain from 'chevrotain';\r\nimport * as AST from './ast';\r\n\r\nimport { ASTNodeType as NT} from './ast';\r\n\r\n  var createToken = chevrotain.createToken;\r\n  var Lexer = chevrotain.Lexer;\r\n  var Parser = chevrotain.Parser;\r\n\r\n\r\n\r\n/*\r\n\r\nvar LogicalOperator = createToken({name: \"AdditionOperator\", pattern: Lexer.NA});\r\nvar And = createToken({name: \"And\", pattern: /and/i, parent: LogicalOperator});\r\nvar Or = createToken({name: \"Or\", pattern: /Or/i, parent: LogicalOperator});\r\n\r\n\r\n// using the NA pattern marks this Token class as 'irrelevant' for the Lexer.\r\n// AdditionOperator defines a Tokens hierarchy but only the leafs in this hierarchy define\r\n// actual Tokens that can appear in the text\r\n\r\n\r\nvar AdditionOperator = createToken({name: \"AdditionOperator\", pattern: Lexer.NA});\r\nvar Plus = createToken({name: \"Plus\", pattern: /\\+/, parent: AdditionOperator});\r\nvar Minus = createToken({name: \"Minus\", pattern: /-/, parent: AdditionOperator});\r\n\r\nvar MultiplicationOperator = createToken({name: \"MultiplicationOperator\", pattern: Lexer.NA});\r\nvar Multi = createToken({name: \"Multi\", pattern: /\\* /, parent: MultiplicationOperator});\r\nvar Div = createToken({name: \"Div\", pattern: /\\//, parent: MultiplicationOperator});\r\n\r\nvar LParen = createToken({name: \"LParen\", pattern: /\\(/});\r\nvar RParen = createToken({name: \"RParen\", pattern: /\\)/});\r\nvar NumberLiteral = createToken({name: \"NumberLiteral\", pattern: /[1-9]\\d* /});\r\n\r\nvar PowerFunc = createToken({name: \"PowerFunc\", pattern: /power/});\r\n\r\n  var List = createToken({name: \"List\", pattern: /LIST/i});\r\n  var Describe = createToken({name: \"Describe\", pattern : /DESCRIBE/i});\r\n  var Is = createToken({name: \"Is\", pattern : /Is/i});\r\n  var What = createToken({name: \"What\", pattern : /What/i});\r\n  var Me = createToken({name: \"Me\", pattern : /Me/i});\r\n  var The = createToken({name: \"The\", pattern : /The/i});\r\n  var Meaning = createToken({name: \"Meaning\", pattern : /Meaning/i});\r\n  var Of = createToken({name: \"Of\", pattern : /Of/i});\r\n  var Relating = createToken({name: \"Relating\", pattern : /RElating/i});\r\n  var All = createToken({name: \"All\", pattern : /All/i});\r\n  var First = createToken({name: \"First\", pattern : /First/i});\r\n  var Oldest = createToken({name: \"Oldest\", pattern : /Oldest/i});\r\n  var Latest = createToken({name: \"Latest\", pattern : /(Latest)|(Newest)/i});\r\n  var In = createToken({name: \"In\", pattern : /In/i});\r\n  var Are = createToken({name: \"Are\", pattern : /Are/i});\r\n  var To = createToken({name: \"To\", pattern : /To/i});\r\n  var With = createToken({name: \"With\", pattern : /With/i});\r\n  var About = createToken({name: \"About\", pattern : /About/i});\r\n  var You = createToken({name: \"You\", pattern : /You/i});\r\n  var AFact = createToken({name: \"AFact\", pattern : /FACT/i});\r\n  var All = createToken({name: \"All\", pattern: /ALL/});\r\n  var Select = createToken({name: \"Select\", pattern: /SELECT/});\r\n  var From = createToken({name: \"From\", pattern: /FROM/});\r\n  var Where = createToken({name: \"Where\", pattern: /WHERE/});\r\n  var Comma = createToken({name: \"Comma\", pattern: /,/});\r\n  var And = createToken({name: \"And\", pattern: /And/i});\r\n  var Every = createToken({name: \"And\", pattern: /And/i});\r\n\r\n  var ACategory = createToken({name: \"ACategory\", pattern: /CAT/});\r\n  var Identifier = createToken({name: \"Identifier\", pattern: /\\w+/});\r\n  var Integer = createToken({name: \"Integer\", pattern: /0|[1-9]\\d+/});\r\n  var GreaterThan = createToken({name: \"GreaterThan\", pattern: /</});\r\n  var LessThan = createToken({name: \"LessThan\", pattern: />/});\r\n*/\r\n\r\n\r\nimport { IFModel as IFModel} from 'mgnlq_model';\r\n\r\n  var WhiteSpace = createToken({name: \"WhiteSpace\", pattern: /\\s+/});\r\n\r\n  WhiteSpace.GROUP = Lexer.SKIPPED;\r\n\r\n  import { Tokens as T }  from './tokens';\r\n  // whitespace is normally very common so it is placed first to speed up the lexer\r\n  var allTokens = Object.keys(T).map(key => T[key]);\r\n\r\n\r\nexport function makeToken(t : IFErBase.IWord, index : number, T : any ) {\r\n  if (!t.rule) {\r\n    throw new Error(\"Token without rule \" + JSON.stringify(t));\r\n  }\r\n  //console.log(Object.keys(T).join(\"\\n\"));\r\n  if (t.rule.wordType === IFModel.WORDTYPE.CATEGORY) {\r\n    return { image : \"CAT\",  startOffset : index, bearer : t, tokenType : T[\"ACategory\"].tokenType };\r\n  };\r\n  if (t.rule.wordType === 'F') {\r\n    //console.log(JSON.stringify(t));\r\n    return { image : \"FACT\",  startOffset : index, bearer : t, tokenType : T[\"AFact\"].tokenType };\r\n  }\r\n  if (t.rule.wordType === 'D') {\r\n    //console.log(JSON.stringify(t));\r\n    return { image : \"DOM\",  startOffset : index, bearer : t, tokenType : T[\"ADomain\"].tokenType };\r\n  }\r\n  if (t.rule.wordType === 'A') {\r\n    //console.log(JSON.stringify(t));\r\n    return { image : \"ANY\",  startOffset : index, bearer : t, tokenType : T[\"AnANY\"].tokenType };\r\n  }\r\n  if (t.rule.wordType === 'M') {\r\n    //console.log(JSON.stringify(t));\r\n    var tlc = t.matchedString.toLowerCase();\r\n    var tlcClean = tlc.replace(/ /g,'_');\r\n    //console.log(\">\" + tlcClean + \"<\");\r\n    //console.log(Object.keys(T).indexOf(\"domain\"));\r\n    //console.log(\">>>\" + JSON.stringify(T[\"domain\"]));\r\n    //console.log(\"> token >>\" + JSON.stringify(T[tlcClean]));\r\n    if (!T[tlcClean]) {\r\n      //console.log(Object.keys(T).join('\\\" \\\"'));\r\n      throw new Error(\"unknown token of type M with \" + t.matchedString);\r\n    }\r\n    //console.log(\" here we go\" + typeof T[\"domain\"]);\r\n    return { image : t.matchedString, bearer : t, startOffset : index, tokenType : T[\"domain\"].tokenType };\r\n  }\r\n  if (t.rule.wordType === 'O') {\r\n    var tlc = t.matchedString.toLowerCase();\r\n    var tlcClean = tlc.replace(/ /g,'_');\r\n    if (!T[tlcClean]) {\r\n      console.log(Object.keys(T).join('\\\" \\\"'));\r\n      throw new Error(\"unknown token of type O with \" + t.matchedString);\r\n      //process.exit(-1);\r\n    }\r\n    return { image : t.matchedString, bearer : t, startOffset : index, tokenType : T[tlcClean].tokenType };\r\n  }\r\n  if (t.rule.wordType === 'I') {\r\n    var tlc = t.matchedString.toLowerCase();\r\n    if (!T[tlc]) {\r\n      console.log(\"unknown token of type I with \" + t.matchedString);\r\n      process.exit(-1);\r\n    }\r\n    return { image : t.matchedString, bearer : t, startOffset : index, tokenType : T[tlc].tokenType };\r\n  }\r\n  throw new Error(\"unknown token \" + JSON.stringify(t));\r\n}\r\n\r\nclass XLexer {\r\n   tokenize = function(sentence : IFErBase.ISentence) : any[]  {\r\n    // console.log(JSON.stringify(sentence));\r\n    return sentence.map( (t,index) => {\r\n         var u =  makeToken(t, index, T);\r\n        debuglog(\"produced nr   \" + index + \" > \" + JSON.stringify(u));\r\n        return u;\r\n    });\r\n  }\r\n};\r\n\r\nexport function getLexer()  : any {\r\n  return new XLexer();\r\n}\r\n\r\n  /* [ AFact, And,\r\n    Describe,\r\n    First, Oldest, Latest, What,\r\n    At, Every, All, At, Least, One,\r\n    The,\r\n    LParen, RParen,\r\n\r\n\r\n   Meaning, Of, Are,  In, About, You, All,\r\n  WhiteSpace, Select, From, Where, Comma, ACategory, All,\r\n    List, Identifier, Integer, GreaterThan, LessThan, To, Relating, With];\r\n*/\r\n    var SelectLexer = new Lexer(allTokens);\r\n\r\n\r\nfunction parse(tokens : any[], startrule : string) {\r\n  const parser = new SelectParser.SelectParser(tokens);\r\n  var res = parser[startrule]();\r\n   if (parser.errors.length > 0) {\r\n    throw new Error('parsing error in  input' + JSON.stringify(parser.errors));\r\n  }\r\n  return res;\r\n}\r\n\r\nexport interface IParsedSentences  extends IFModel.IProcessedSentences {\r\n  asts : AST.ASTNode[],\r\n  domains? : string[]\r\n};\r\n\r\nexport declare const ERR_PARSE_ERROR = \"PARSE_ERROR\";\r\n\r\nexport function parseSentenceToAsts(s : string, model : IFModel.IModels, words : any) : IParsedSentences {\r\n  var res = ErBase.processString(s, model.rules, words);\r\n  debuglog(function() { 'res > ' + JSON.stringify(res, undefined, 2)});\r\n  var res2 = Object.assign({}, res) as IParsedSentences;\r\n  res2.asts = res.sentences.map((sentence, index) => {\r\n    var lexingResult = getLexer().tokenize(sentence);\r\n    debuglog( () => {\r\n      var sStrings = lexingResult.map((t, indext) =>  `[${indext}] ${t.image} (${t.bearer && t.bearer.matchedString || JSON.stringify(sentence[index][t.startIndex])})` );\r\n      return 'tokens: #' + index + '...\\n' + sStrings.join('\\n');}\r\n    );\r\n    //test.deepEqual(sStrings, ['CAT', 'CAT', 'CAT', 'CAT', 'with', 'CAT', 'FACT', 'CAT', 'FACT', 'FACT' ]);\r\n    try {\r\n      var ast = parse(lexingResult, 'catListOpMore');\r\n      debuglog(() => {\r\n        return 'ast: #' + index + '...\\n' + AST.astToText(ast);\r\n      });\r\n      return ast;\r\n    } catch (e) {\r\n      debuglog(()=>' here the error ' + Object.keys(e));\r\n      res2.errors = res2.errors || [];\r\n      debuglog('parse error ' + e.toString());\r\n\r\n      res2.errors[index] = {\r\n        err_code : ERR_PARSE_ERROR,\r\n        text : e.toString().split(',\\\"token\\\":')[0]\r\n      }  as IFErBase.IERError;\r\n    }\r\n    return undefined;\r\n  });\r\n  return res2;\r\n}\r\n//\r\nexport {\r\n   SelectLexer,\r\n   parse\r\n   // defaultRule : \"selectStatement\"\r\n};\r\n"],"sourceRoot":"ABC"}